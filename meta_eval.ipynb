{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata: there are 4 datasets, 25 base LLMs and 18 evaluation protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    \"llmbar_natural\",\n",
    "    \"llmbar_adversarial\",\n",
    "    \"mtbench\",\n",
    "    \"instrusum\",\n",
    "]  # 4 datasets\n",
    "\n",
    "LLMS = [\n",
    "    \"llama-3.1-405b\",\n",
    "    \"llama-3.1-70b\",\n",
    "    \"llama-3.1-8b\",\n",
    "    \"llama-3-70b\",\n",
    "    \"llama-3-8b\",\n",
    "    \"llama-2-70b\",\n",
    "    \"llama-2-13b\",\n",
    "    \"llama-2-7b\",\n",
    "    \"tulu-2-dpo-70b\",\n",
    "    \"tulu-2-70b\",\n",
    "    \"tulu-2-dpo-13b\",\n",
    "    \"tulu-2-13b\",\n",
    "    \"tulu-2-dpo-7b\",\n",
    "    \"tulu-2-7b\",\n",
    "    \"mixtral-8x7b\",\n",
    "    \"mistral-7b-v0.3\",\n",
    "    \"qwen-2.5-72b\",\n",
    "    \"qwen-2-72b\",\n",
    "    \"qwen-1.5-72b\",\n",
    "    \"qwen-1.5-32b\",\n",
    "    \"gemma-7b\",\n",
    "    \"gemma-2b\",\n",
    "    \"glm-4-9b\",\n",
    "    \"yi-1.5-34b\",\n",
    "    \"yi-1.5-9b\",\n",
    "]  # 25 llms\n",
    "\n",
    "PROTOCOLS = [\n",
    "    \"base\",\n",
    "    \"cot\",\n",
    "    \"metric\",\n",
    "    \"reference\",\n",
    "    \"metric+reference\",\n",
    "    \"swap&synthesize\",\n",
    "    \"fine-grained-diff\",\n",
    "    \"multi-role-round1\",\n",
    "    \"multi-role-round2\",\n",
    "    \"multi-aspect-two\",\n",
    "    \"multi-aspect-single\",\n",
    "    \"gpt4-reference\",\n",
    "    \"prepair\",\n",
    "    \"cot&consistency\",\n",
    "    \"protocol-consistency\",\n",
    "]  # 15 protocols\n",
    "\n",
    "BENCHMARK_PROTOCOLS = [\n",
    "    \"alpacaeval\",\n",
    "    \"arena_hard\",\n",
    "    \"wildbench\",\n",
    "]  # 3 benchmark protocols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data = load_dataset(\"yale-nlp/ReIFE\", \"src\")\n",
    "predictions = load_dataset(\"yale-nlp/ReIFE\", \"predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['winner', 'output_1', 'instruction', 'output_2'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "src_data[\"llmbar_natural\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base_LLM', 'evaluation_protocol', 'predictions', 'swap_predictions'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[\"llmbar_natural\"][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform meta-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_compare(\n",
    "    evaluator1_responses: list,\n",
    "    evaluator2_responses: list,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compare pairwise evaluators.\n",
    "\n",
    "    Args:\n",
    "        evaluator1: The responses from the first evaluator.\n",
    "        evaluator2: The responses from the second evaluator.\n",
    "\n",
    "    Returns:\n",
    "        float: The evaluation accuracy.\n",
    "    \"\"\"\n",
    "    assert len(evaluator1_responses) == len(evaluator2_responses)\n",
    "    evaluator1_winners = np.array(evaluator1_responses)\n",
    "    evaluator2_winners = np.array(evaluator2_responses)\n",
    "    acc = (evaluator1_winners == evaluator2_winners).mean().item()\n",
    "    return acc\n",
    "\n",
    "\n",
    "def pairwise_meta_eval(\n",
    "    human: list,\n",
    "    model: str | list,\n",
    "    model_swap: list | None = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate a pairwise evaluator.\n",
    "\n",
    "    Args:\n",
    "        human: The responses from the human evaluator.\n",
    "        model: The responses from the model evaluator.\n",
    "        model_swap: The responses from the model evaluator with swapped winners.\n",
    "\n",
    "    Returns:\n",
    "        float: The evaluation accuracy.\n",
    "    \"\"\"\n",
    "    acc = pairwise_compare(human, model)\n",
    "    if model_swap is not None:\n",
    "        swap_acc = pairwise_compare(human, model_swap)\n",
    "        acc = (acc + swap_acc) / 2\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {dataset: [] for dataset in DATASETS}\n",
    "\n",
    "for dataset in tqdm(DATASETS):\n",
    "    human_annotations = src_data[dataset][\"winner\"]\n",
    "    for prediction in tqdm(predictions[dataset]):\n",
    "        model_predictions = prediction[\"predictions\"]\n",
    "        model_swap_predictions = prediction[\"swap_predictions\"]\n",
    "        acc = pairwise_meta_eval(\n",
    "            human_annotations, model_predictions, model_swap_predictions\n",
    "        )\n",
    "        results[dataset].append(\n",
    "            {\n",
    "                \"acc\": acc,\n",
    "                \"base_LLM\": prediction[\"base_LLM\"],\n",
    "                \"evaluation_protocol\": prediction[\"evaluation_protocol\"],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average performance of the 25 base LLMs across the 15 evaluation protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 4652.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model              llmbar_natural    llmbar_adversarial    mtbench    instrusum    avg\n",
      "---------------  ----------------  --------------------  ---------  -----------  -----\n",
      "llama-3.1-405b              94.13                 81.31      81.42        80.05  84.23\n",
      "llama-3.1-70b               91.73                 80.23      81.02        75.30  82.07\n",
      "qwen-2-72b                  91.57                 69.85      82.20        72.77  79.10\n",
      "qwen-2.5-72b                89.57                 71.01      81.20        72.44  78.56\n",
      "llama-3-70b                 88.20                 71.49      80.03        74.35  78.52\n",
      "qwen-1.5-72b                86.10                 56.02      76.40        67.98  71.62\n",
      "yi-1.5-34b                  86.53                 57.61      73.85        66.03  71.01\n",
      "tulu-2-dpo-70b              84.23                 56.13      73.70        66.53  70.15\n",
      "mixtral-8x7b                82.20                 54.73      73.78        66.44  69.29\n",
      "tulu-2-70b                  83.60                 54.95      72.82        65.39  69.19\n",
      "qwen-1.5-32b                85.70                 47.86      76.98        64.28  68.71\n",
      "glm-4-9b                    79.13                 54.38      70.58        68.18  68.07\n",
      "yi-1.5-9b                   77.13                 55.71      71.30        60.09  66.06\n",
      "llama-3.1-8b                75.33                 54.67      70.33        62.27  65.65\n",
      "llama-3-8b                  71.83                 47.94      71.43        62.59  63.45\n",
      "llama-2-70b                 74.40                 36.64      68.47        63.33  60.71\n",
      "mistral-7b-v0.3             65.13                 47.31      66.58        61.93  60.24\n",
      "tulu-2-dpo-13b              66.00                 39.98      65.33        60.29  57.90\n",
      "tulu-2-13b                  63.33                 39.71      65.40        60.24  57.17\n",
      "llama-2-13b                 63.93                 36.32      62.65        56.88  54.95\n",
      "tulu-2-dpo-7b               58.17                 43.69      57.17        57.23  54.06\n",
      "gemma-7b                    51.90                 42.71      59.22        56.66  52.62\n",
      "tulu-2-7b                   49.07                 45.82      54.87        56.90  51.66\n",
      "llama-2-7b                  48.63                 45.27      54.72        54.40  50.75\n",
      "gemma-2b                    44.60                 47.03      53.83        55.54  50.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm_results = {llm: {dataset: [] for dataset in DATASETS} for llm in LLMS}\n",
    "protocols = set(PROTOCOLS)\n",
    "\n",
    "for llm in tqdm(LLMS):\n",
    "    for dataset in DATASETS:\n",
    "        for result in results[dataset]:\n",
    "            if result[\"base_LLM\"] == llm and result[\"evaluation_protocol\"] in protocols:\n",
    "                llm_results[llm][dataset].append(result[\"acc\"] * 100)\n",
    "        llm_results[llm][dataset] = np.mean(llm_results[llm][dataset])\n",
    "    llm_results[llm][\"avg\"] = np.mean(list(llm_results[llm].values()))\n",
    "\n",
    "table = []\n",
    "for llm in LLMS:\n",
    "    table.append([llm] + [llm_results[llm][dataset] for dataset in DATASETS + [\"avg\"]])\n",
    "table = sorted(table, key=lambda x: x[-1], reverse=True)\n",
    "print(tabulate(table, floatfmt=\".02f\", headers=[\"Model\"] + DATASETS + [\"avg\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average performance of the 15 evaluation protocols + 3 benchmark protocols across the 25 base LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 5247.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocol                llmbar_natural    llmbar_adversarial    mtbench    instrusum    avg\n",
      "--------------------  ----------------  --------------------  ---------  -----------  -----\n",
      "prepair                          76.44                 61.77      69.72        63.82  67.94\n",
      "gpt4-reference                   76.66                 57.97      70.12        65.99  67.68\n",
      "metric+reference                 76.64                 58.25      69.98        65.62  67.62\n",
      "protocol-consistency             76.34                 55.94      70.89        66.09  67.31\n",
      "metric                           75.84                 56.22      70.74        65.67  67.12\n",
      "reference                        76.16                 57.47      69.44        65.22  67.07\n",
      "swap&synthesize                  75.62                 54.38      70.82        66.22  66.76\n",
      "cot&consistency                  74.94                 54.08      70.55        65.38  66.24\n",
      "base                             74.72                 53.53      70.67        65.99  66.23\n",
      "cot                              73.58                 53.56      70.18        64.90  65.56\n",
      "arena_hard                       76.34                 46.15      72.12        64.48  64.77\n",
      "wildbench                        74.94                 47.45      70.70        63.05  64.04\n",
      "multi-aspect-two                 77.12                 42.31      72.54        62.33  63.57\n",
      "fine-grained-diff                71.22                 49.50      69.20        61.83  62.94\n",
      "multi-role-round1                68.00                 53.90      66.17        61.62  62.42\n",
      "multi-role-round2                68.40                 53.37      65.66        61.73  62.29\n",
      "multi-aspect-single              69.62                 40.78      70.49        62.44  60.83\n",
      "alpacaeval                       65.30                 49.78      63.07        59.36  59.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "protocols = PROTOCOLS + BENCHMARK_PROTOCOLS\n",
    "protocol_results = {protocol: {dataset: [] for dataset in DATASETS} for protocol in protocols}\n",
    "\n",
    "for protocol in tqdm(protocols):\n",
    "    for dataset in DATASETS:\n",
    "        for result in results[dataset]:\n",
    "            if result[\"evaluation_protocol\"] == protocol:\n",
    "                protocol_results[protocol][dataset].append(result[\"acc\"] * 100)\n",
    "        protocol_results[protocol][dataset] = np.mean(protocol_results[protocol][dataset])\n",
    "    protocol_results[protocol][\"avg\"] = np.mean(list(protocol_results[protocol].values()))\n",
    "\n",
    "table = []\n",
    "for protocol in protocols:\n",
    "    table.append([protocol] + [protocol_results[protocol][dataset] for dataset in DATASETS + [\"avg\"]])\n",
    "table = sorted(table, key=lambda x: x[-1], reverse=True)\n",
    "print(tabulate(table, floatfmt=\".02f\", headers=[\"Protocol\"] + DATASETS + [\"avg\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
